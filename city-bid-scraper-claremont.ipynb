{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOodMpJvFNZrHqO84bGwLDA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# City Bid Tracker - Claremont\n","\n","Automated scraper for public procurement opportunities from Claremont's official website.\n","\n","## Purpose\n","Helps contractors and vendors discover bidding opportunities by extracting:\n","- Bid titles and descriptions\n","- Closing dates and status\n","- Direct links to full documentation\n","\n","## Setup & Usage\n","1. Run the dependency installation cell\n","2. Execute the crawler class definition\n","3. Run the final execution cell\n","4. CSV file will be automatically downloaded\n","\n","## Output\n","Creates `claremont_bids.csv` with all current bid opportunities.\n","\n","## Technical Notes\n","This crawler handles a unique website structure that differs from standard bidding platforms, using custom CSS selectors for list containers instead of traditional table parsing."],"metadata":{"id":"CRwCjDoDJ-3C"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BBjSsSKdEhrD","executionInfo":{"status":"ok","timestamp":1740067789764,"user_tz":300,"elapsed":43357,"user":{"displayName":"Ritunjay Murali","userId":"07921326813984298862"}},"outputId":"975ce748-0e14-4443-aa28-7a46aea96f6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting selenium\n","  Downloading selenium-4.29.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting webdriver_manager\n","  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n","Collecting trio~=0.17 (from selenium)\n","  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n","Collecting trio-websocket~=0.9 (from selenium)\n","  Downloading trio_websocket-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n","Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n","Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver_manager) (2.32.3)\n","Collecting python-dotenv (from webdriver_manager)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver_manager) (24.2)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.1.0)\n","Collecting sortedcontainers (from trio~=0.17->selenium)\n","  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n","Collecting outcome (from trio~=0.17->selenium)\n","  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n","Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n","  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (3.4.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n","Downloading selenium-4.29.0-py3-none-any.whl (9.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n","Downloading trio-0.29.0-py3-none-any.whl (492 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio_websocket-0.12.1-py3-none-any.whl (21 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n","Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n","Installing collected packages: sortedcontainers, wsproto, python-dotenv, outcome, webdriver_manager, trio, trio-websocket, selenium\n","Successfully installed outcome-1.3.0.post0 python-dotenv-1.0.1 selenium-4.29.0 sortedcontainers-2.4.0 trio-0.29.0 trio-websocket-0.12.1 webdriver_manager-4.0.2 wsproto-1.2.0\n","Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,317 kB]\n","Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,661 kB]\n","Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,692 kB]\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,230 kB]\n","Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,610 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,793 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,526 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,939 kB]\n","Fetched 25.2 MB in 5s (4,932 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  apparmor chromium-browser libfuse3-3 liblzo2-2 snapd squashfs-tools systemd-hwe-hwdb udev\n","Suggested packages:\n","  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n","The following NEW packages will be installed:\n","  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n","  systemd-hwe-hwdb udev\n","0 upgraded, 9 newly installed, 0 to remove and 30 not upgraded.\n","Need to get 30.1 MB of archives.\n","After this operation, 123 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.66.1+22.04 [27.6 MB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n","Fetched 30.1 MB in 0s (64.8 MB/s)\n","Preconfiguring packages ...\n","Selecting previously unselected package apparmor.\n","(Reading database ... 124926 files and directories currently installed.)\n","Preparing to unpack .../0-apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n","Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n","Selecting previously unselected package liblzo2-2:amd64.\n","Preparing to unpack .../1-liblzo2-2_2.10-2build3_amd64.deb ...\n","Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n","Selecting previously unselected package squashfs-tools.\n","Preparing to unpack .../2-squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n","Unpacking squashfs-tools (1:4.5-3build1) ...\n","Selecting previously unselected package udev.\n","Preparing to unpack .../3-udev_249.11-0ubuntu3.12_amd64.deb ...\n","Unpacking udev (249.11-0ubuntu3.12) ...\n","Selecting previously unselected package libfuse3-3:amd64.\n","Preparing to unpack .../4-libfuse3-3_3.10.5-1build1_amd64.deb ...\n","Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n","Selecting previously unselected package snapd.\n","Preparing to unpack .../5-snapd_2.66.1+22.04_amd64.deb ...\n","Unpacking snapd (2.66.1+22.04) ...\n","Setting up apparmor (3.0.4-2ubuntu2.4) ...\n","Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n","Setting up liblzo2-2:amd64 (2.10-2build3) ...\n","Setting up squashfs-tools (1:4.5-3build1) ...\n","Setting up udev (249.11-0ubuntu3.12) ...\n","invoke-rc.d: could not determine current runlevel\n","invoke-rc.d: policy-rc.d denied execution of start.\n","Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n","Setting up snapd (2.66.1+22.04) ...\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n","Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n","Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n","Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n","Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n","Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n","Selecting previously unselected package chromium-browser.\n","(Reading database ... 125363 files and directories currently installed.)\n","Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n","=> Installing the chromium snap\n","==> Checking connectivity with the snap store\n","===> System doesn't have a working snapd, skipping\n","Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Selecting previously unselected package chromium-chromedriver.\n","Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n","Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Selecting previously unselected package systemd-hwe-hwdb.\n","Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n","Unpacking systemd-hwe-hwdb (249.11.5) ...\n","Setting up systemd-hwe-hwdb (249.11.5) ...\n","Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Processing triggers for udev (249.11-0ubuntu3.12) ...\n","Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n"]}],"source":["!pip install selenium webdriver_manager pandas\n","\n","# Install Chrome and ChromeDriver\n","!apt-get update\n","!apt install chromium-chromedriver"]},{"cell_type":"code","source":["from selenium import webdriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from selenium.webdriver.chrome.service import Service\n","from selenium.webdriver.chrome.options import Options\n","from webdriver_manager.chrome import ChromeDriverManager\n","from datetime import datetime\n","import csv\n","import os\n","import time\n","from google.colab import files\n","import random\n","\n","class ClaremontBidsCrawler:\n","    def __init__(self):\n","        self.base_url = \"https://www.claremontca.gov/Business-Development/City-Vending/Bids-and-Tenders-Current\"\n","        self.output_file = \"claremont_bids.csv\"\n","        self.fieldnames = [\n","            \"Category\",\n","            \"Bid Title\",\n","            \"Description\",\n","            \"Closing Date\",\n","            \"Status\",\n","            \"Details URL\",\n","            \"Last Updated\"\n","        ]\n","        self.max_retries = 3\n","        self.setup_driver()\n","\n","    def setup_driver(self):\n","        \"\"\"Setup Chrome driver with enhanced options\"\"\"\n","        chrome_options = Options()\n","        chrome_options.add_argument('--headless')\n","        chrome_options.add_argument('--no-sandbox')\n","        chrome_options.add_argument('--disable-dev-shm-usage')\n","        chrome_options.add_argument('--disable-gpu')\n","        chrome_options.add_argument('--window-size=1920,1080')\n","\n","        # Add realistic browser headers\n","        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n","        chrome_options.add_argument('--accept-language=en-US,en;q=0.9')\n","        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n","\n","        try:\n","            print(\"Attempting to use system chromedriver...\")\n","            self.driver = webdriver.Chrome(options=chrome_options)\n","        except Exception as e:\n","            print(f\"System chromedriver failed: {str(e)}\")\n","            print(\"Attempting to use ChromeDriverManager...\")\n","            service = Service(ChromeDriverManager().install())\n","            self.driver = webdriver.Chrome(service=service, options=chrome_options)\n","\n","        self.driver.set_page_load_timeout(30)\n","        self.wait = WebDriverWait(self.driver, 15)\n","        print(\"Chrome driver initialized successfully\")\n","\n","    def random_delay(self):\n","        \"\"\"Add random delay between actions\"\"\"\n","        time.sleep(random.uniform(2, 5))\n","\n","    def parse_bid_item(self, item, category=\"\"):\n","        \"\"\"Parse individual bid listing\"\"\"\n","        try:\n","            print(\"\\nParsing new bid item...\")\n","            bid_data = {\n","                \"Category\": category,\n","                \"Bid Title\": \"\",\n","                \"Description\": \"\",\n","                \"Closing Date\": \"\",\n","                \"Status\": \"\",\n","                \"Details URL\": \"\",\n","                \"Last Updated\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","            }\n","\n","            try:\n","                # Get title, URL and description\n","                title_elem = item.find_element(By.CSS_SELECTOR, \"h2.list-item-title\")\n","                bid_data[\"Bid Title\"] = title_elem.text.strip()\n","\n","                link_elem = item.find_element(By.CSS_SELECTOR, \"article a\")\n","                bid_data[\"Details URL\"] = link_elem.get_attribute(\"href\")\n","\n","                desc_elem = item.find_element(By.CSS_SELECTOR, \"article p:not([class])\")\n","                if desc_elem:\n","                    bid_data[\"Description\"] = desc_elem.text.strip()\n","\n","                # Get status and closing date\n","                status_elem = item.find_element(By.CSS_SELECTOR, \"p.status-list\")\n","                if status_elem:\n","                    status_text = status_elem.text.strip()\n","                    if \"Status:\" in status_text:\n","                        bid_data[\"Status\"] = status_text.split(\"Status:\")[1].strip()\n","\n","                closing_elem = item.find_element(By.CSS_SELECTOR, \"p.applications-closing\")\n","                if closing_elem:\n","                    closing_text = closing_elem.text.strip()\n","                    if \"Closed\" in closing_text:\n","                        bid_data[\"Closing Date\"] = closing_text.replace(\"Closed\", \"\").strip()\n","\n","            except Exception as e:\n","                print(f\"Error parsing bid elements: {str(e)}\")\n","                return None\n","\n","            print(f\"Parsed bid: {bid_data['Bid Title']}\")\n","            return bid_data if bid_data[\"Bid Title\"] else None\n","\n","        except Exception as e:\n","            print(f\"Error parsing bid item: {str(e)}\")\n","            return None\n","\n","    def setup_csv(self):\n","        \"\"\"Create or verify CSV file with headers\"\"\"\n","        try:\n","            if not os.path.exists(self.output_file):\n","                with open(self.output_file, 'w', newline='', encoding='utf-8') as f:\n","                    writer = csv.DictWriter(f, fieldnames=self.fieldnames)\n","                    writer.writeheader()\n","                print(f\"Created new CSV file: {self.output_file}\")\n","            else:\n","                print(f\"CSV file already exists: {self.output_file}\")\n","        except Exception as e:\n","            print(f\"Error setting up CSV: {str(e)}\")\n","\n","    def get_page_with_retry(self):\n","        \"\"\"Attempt to load the page with retries\"\"\"\n","        for attempt in range(self.max_retries):\n","            try:\n","                print(f\"\\nAttempt {attempt + 1} to load page...\")\n","                self.driver.get(self.base_url)\n","                self.random_delay()\n","\n","                # Check if page loaded successfully\n","                try:\n","                    self.wait.until(\n","                        EC.presence_of_element_located((By.CLASS_NAME, \"list-container\"))\n","                    )\n","                except Exception as e:\n","                    print(f\"Error waiting for page load: {str(e)}\")\n","                    continue\n","\n","                print(\"Page loaded successfully\")\n","                return True\n","\n","            except Exception as e:\n","                print(f\"Error loading page: {str(e)}\")\n","                if attempt < self.max_retries - 1:\n","                    wait_time = (attempt + 1) * 5\n","                    print(f\"Waiting {wait_time} seconds before retry...\")\n","                    time.sleep(wait_time)\n","                continue\n","        return False\n","\n","    def get_bid_listings(self):\n","        \"\"\"Fetch and parse all bid listings\"\"\"\n","        try:\n","            if not self.get_page_with_retry():\n","                print(\"Failed to load page after all retries\")\n","                return []\n","\n","            print(\"Looking for bid listings...\")\n","            bids = []\n","\n","            # Find the list container and all bid items\n","            list_container = self.wait.until(\n","                EC.presence_of_element_located((By.CLASS_NAME, \"list-container\"))\n","            )\n","\n","            bid_items = list_container.find_elements(By.CLASS_NAME, \"list-item-container\")\n","            print(f\"Found {len(bid_items)} bid items\")\n","\n","            for item in bid_items:\n","                bid_data = self.parse_bid_item(item)\n","                if bid_data:\n","                    bids.append(bid_data)\n","                self.random_delay()\n","\n","            print(f\"Successfully parsed {len(bids)} bids\")\n","            return bids\n","\n","        except Exception as e:\n","            print(f\"Error fetching bid listings: {str(e)}\")\n","            return []\n","\n","    def save_bids(self, bids):\n","        \"\"\"Save bid data to CSV\"\"\"\n","        try:\n","            if not bids:\n","                print(\"No bids to save\")\n","                return\n","\n","            existing_bids = set()\n","            if os.path.exists(self.output_file):\n","                with open(self.output_file, 'r', encoding='utf-8') as f:\n","                    reader = csv.DictReader(f)\n","                    for row in reader:\n","                        existing_bids.add(f\"{row['Bid Title']}-{row['Closing Date']}\")\n","\n","            new_bids = [\n","                bid for bid in bids\n","                if f\"{bid['Bid Title']}-{bid['Closing Date']}\" not in existing_bids\n","            ]\n","\n","            if new_bids:\n","                mode = 'w' if not os.path.exists(self.output_file) else 'a'\n","                with open(self.output_file, mode, newline='', encoding='utf-8') as f:\n","                    writer = csv.DictWriter(f, fieldnames=self.fieldnames)\n","                    if mode == 'w':\n","                        writer.writeheader()\n","                    writer.writerows(new_bids)\n","                print(f\"Added {len(new_bids)} new bids\")\n","            else:\n","                print(\"No new bids to add\")\n","\n","            # Download the CSV file\n","            files.download(self.output_file)\n","\n","        except Exception as e:\n","            print(f\"Error saving bids: {str(e)}\")\n","\n","    def run(self):\n","        \"\"\"Main execution method\"\"\"\n","        try:\n","            print(f\"Starting Claremont bids crawler at {datetime.now()}\")\n","            self.setup_csv()\n","            bids = self.get_bid_listings()\n","            self.save_bids(bids)\n","            print(\"Crawler execution completed\")\n","        finally:\n","            if hasattr(self, 'driver'):\n","                self.driver.quit()\n","\n","# Example usage:\n","# crawler = ClaremontBidsCrawler()\n","# crawler.run()"],"metadata":{"id":"NYz1gJ67HoBp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["crawler = ClaremontBidsCrawler()\n","crawler.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UN8RQamoHvds","outputId":"690f06c0-5958-4277-fb17-5b784d333e53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to use system chromedriver...\n","Chrome driver initialized successfully\n","Starting Claremont bids crawler at 2025-02-20 16:10:14.474414\n","Created new CSV file: claremont_bids.csv\n","\n","Attempt 1 to load page...\n","Page loaded successfully\n","Looking for bid listings...\n","Found 2 bid items\n","\n","Parsing new bid item...\n","Parsed bid: Sidewalk Rehabilitation Project\n"]}]},{"cell_type":"markdown","source":["## Disclaimer\n","This tool accesses publicly available information only from official government websites. It respects robots.txt guidelines and implements responsible scraping practices with delays between requests."],"metadata":{"id":"4GNFcV5fJ7wY"}}]}