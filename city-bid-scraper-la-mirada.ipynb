{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOSkpswAtW7jZXSHD3bgJWG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# City Bid Tracker - La Mirada\n","\n","Automated scraper for public procurement opportunities from La Mirada's official website.\n","\n","## Purpose\n","Helps contractors and vendors discover bidding opportunities by extracting:\n","- RFP/RFQ titles and types\n","- Due dates and additional information\n","- Direct links to full documentation\n","\n","## Setup & Usage\n","1. Run the dependency installation cell\n","2. Execute the crawler class definition\n","3. Run the final execution cell\n","4. CSV file will be automatically downloaded\n","\n","## Output\n","Creates `la_mirada_bids.csv` with all current opportunities and bid results.\n","\n","## Technical Notes\n","This crawler implements dual parsing logic to handle both \"Bid Results\" sections and RFP tables on the same page, accommodating the city's unique content structure."],"metadata":{"id":"sFZsef6oKnlU"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fjvvc7fIMSL4","executionInfo":{"status":"ok","timestamp":1740079636624,"user_tz":300,"elapsed":32694,"user":{"displayName":"Ritunjay Murali","userId":"07921326813984298862"}},"outputId":"f5473c23-92d5-486a-b8f5-1e4cbc8d86fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting selenium\n","  Downloading selenium-4.29.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting webdriver_manager\n","  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n","Collecting trio~=0.17 (from selenium)\n","  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n","Collecting trio-websocket~=0.9 (from selenium)\n","  Downloading trio_websocket-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n","Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n","Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver_manager) (2.32.3)\n","Collecting python-dotenv (from webdriver_manager)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver_manager) (24.2)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.1.0)\n","Collecting sortedcontainers (from trio~=0.17->selenium)\n","  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n","Collecting outcome (from trio~=0.17->selenium)\n","  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n","Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n","  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (3.4.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n","Downloading selenium-4.29.0-py3-none-any.whl (9.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n","Downloading trio-0.29.0-py3-none-any.whl (492 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio_websocket-0.12.1-py3-none-any.whl (21 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n","Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n","Installing collected packages: sortedcontainers, wsproto, python-dotenv, outcome, webdriver_manager, trio, trio-websocket, selenium\n","Successfully installed outcome-1.3.0.post0 python-dotenv-1.0.1 selenium-4.29.0 sortedcontainers-2.4.0 trio-0.29.0 trio-websocket-0.12.1 webdriver_manager-4.0.2 wsproto-1.2.0\n","Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n","Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,661 kB]\n","Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,692 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,526 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,939 kB]\n","Fetched 16.2 MB in 3s (6,039 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  apparmor chromium-browser libfuse3-3 liblzo2-2 snapd squashfs-tools systemd-hwe-hwdb udev\n","Suggested packages:\n","  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n","The following NEW packages will be installed:\n","  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n","  systemd-hwe-hwdb udev\n","0 upgraded, 9 newly installed, 0 to remove and 25 not upgraded.\n","Need to get 30.1 MB of archives.\n","After this operation, 123 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.66.1+22.04 [27.6 MB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n","Fetched 30.1 MB in 1s (42.2 MB/s)\n","Preconfiguring packages ...\n","Selecting previously unselected package apparmor.\n","(Reading database ... 124926 files and directories currently installed.)\n","Preparing to unpack .../0-apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n","Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n","Selecting previously unselected package liblzo2-2:amd64.\n","Preparing to unpack .../1-liblzo2-2_2.10-2build3_amd64.deb ...\n","Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n","Selecting previously unselected package squashfs-tools.\n","Preparing to unpack .../2-squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n","Unpacking squashfs-tools (1:4.5-3build1) ...\n","Selecting previously unselected package udev.\n","Preparing to unpack .../3-udev_249.11-0ubuntu3.12_amd64.deb ...\n","Unpacking udev (249.11-0ubuntu3.12) ...\n","Selecting previously unselected package libfuse3-3:amd64.\n","Preparing to unpack .../4-libfuse3-3_3.10.5-1build1_amd64.deb ...\n","Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n","Selecting previously unselected package snapd.\n","Preparing to unpack .../5-snapd_2.66.1+22.04_amd64.deb ...\n","Unpacking snapd (2.66.1+22.04) ...\n","Setting up apparmor (3.0.4-2ubuntu2.4) ...\n","Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n","Setting up liblzo2-2:amd64 (2.10-2build3) ...\n","Setting up squashfs-tools (1:4.5-3build1) ...\n","Setting up udev (249.11-0ubuntu3.12) ...\n","invoke-rc.d: could not determine current runlevel\n","invoke-rc.d: policy-rc.d denied execution of start.\n","Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n","Setting up snapd (2.66.1+22.04) ...\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n","Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n","Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n","Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n","Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n","Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n","Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n","Selecting previously unselected package chromium-browser.\n","(Reading database ... 125363 files and directories currently installed.)\n","Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n","=> Installing the chromium snap\n","==> Checking connectivity with the snap store\n","===> System doesn't have a working snapd, skipping\n","Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Selecting previously unselected package chromium-chromedriver.\n","Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n","Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Selecting previously unselected package systemd-hwe-hwdb.\n","Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n","Unpacking systemd-hwe-hwdb (249.11.5) ...\n","Setting up systemd-hwe-hwdb (249.11.5) ...\n","Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n","Processing triggers for udev (249.11-0ubuntu3.12) ...\n","Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n"]}],"source":["!pip install selenium webdriver_manager pandas\n","\n","# Install Chrome and ChromeDriver\n","!apt-get update\n","!apt install chromium-chromedriver"]},{"cell_type":"code","source":["from selenium import webdriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from selenium.webdriver.chrome.service import Service\n","from selenium.webdriver.chrome.options import Options\n","from webdriver_manager.chrome import ChromeDriverManager\n","from datetime import datetime\n","import csv\n","import os\n","import time\n","import random\n","from google.colab import files\n","\n","class LaMiradaBidsCrawler:\n","    def __init__(self):\n","        self.base_url = \"https://www.cityoflamirada.org/about-us/city-clerk/city-bids-rfps-rfqs\"\n","        self.output_file = \"la_mirada_bids.csv\"\n","        self.fieldnames = [\n","            \"Type\",\n","            \"Title\",\n","            \"Due Date\",\n","            \"Additional Info\",\n","            \"Details URL\",\n","            \"Last Updated\"\n","        ]\n","        self.max_retries = 3\n","        self.setup_driver()\n","\n","    def setup_driver(self):\n","        \"\"\"Setup Chrome driver with enhanced options\"\"\"\n","        chrome_options = Options()\n","        chrome_options.add_argument('--headless')\n","        chrome_options.add_argument('--no-sandbox')\n","        chrome_options.add_argument('--disable-dev-shm-usage')\n","        chrome_options.add_argument('--disable-gpu')\n","        chrome_options.add_argument('--window-size=1920,1080')\n","\n","        # Add realistic browser headers\n","        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n","        chrome_options.add_argument('--accept-language=en-US,en;q=0.9')\n","        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n","\n","        try:\n","            print(\"Attempting to use system chromedriver...\")\n","            self.driver = webdriver.Chrome(options=chrome_options)\n","        except Exception as e:\n","            print(f\"System chromedriver failed: {str(e)}\")\n","            print(\"Attempting to use ChromeDriverManager...\")\n","            service = Service(ChromeDriverManager().install())\n","            self.driver = webdriver.Chrome(service=service, options=chrome_options)\n","\n","        self.driver.set_page_load_timeout(30)\n","        self.wait = WebDriverWait(self.driver, 15)\n","        print(\"Chrome driver initialized successfully\")\n","\n","    def random_delay(self):\n","        \"\"\"Add random delay between actions\"\"\"\n","        time.sleep(random.uniform(2, 5))\n","\n","    def parse_bid_results(self):\n","        \"\"\"Parse the Bid Results section\"\"\"\n","        bids = []\n","        try:\n","            # Find bid results section and links\n","            bid_results_section = self.driver.find_element(By.CSS_SELECTOR, \"h2.titlewidget-title\")\n","            if \"Bid Results\" in bid_results_section.text:\n","                # Look for links in the following elements\n","                bid_links = self.driver.find_elements(By.CSS_SELECTOR, \"p a[href*='showpublisheddocument']\")\n","                for link in bid_links:\n","                    title = link.text.strip()\n","                    if title:\n","                        bid_data = {\n","                            \"Type\": \"Bid Result\",\n","                            \"Title\": title,\n","                            \"Due Date\": \"Closed\",  # Past bids\n","                            \"Additional Info\": \"\",\n","                            \"Details URL\": link.get_attribute(\"href\"),\n","                            \"Last Updated\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","                        }\n","                        bids.append(bid_data)\n","                        print(f\"Parsed bid result: {bid_data['Title']}\")\n","        except Exception as e:\n","            print(f\"Error parsing bid results: {str(e)}\")\n","        return bids\n","\n","    def parse_rfp_table(self):\n","        \"\"\"Parse the RFPs and RFQs table\"\"\"\n","        bids = []\n","        try:\n","            # Skip header row and find actual RFP entries\n","            table = self.driver.find_element(By.CSS_SELECTOR, \"table\")\n","            rows = table.find_elements(By.CSS_SELECTOR, \"tbody tr\")\n","\n","            # Skip header row if it exists\n","            data_rows = rows[1:] if len(rows) > 0 else []\n","\n","            for row in data_rows:\n","                cells = row.find_elements(By.TAG_NAME, \"td\")\n","                if len(cells) >= 2:\n","                    # First cell contains RFP title and additional documents\n","                    rfp_cell = cells[0]\n","                    links = rfp_cell.find_elements(By.TAG_NAME, \"a\")\n","\n","                    if not links:\n","                        continue\n","\n","                    # First link is the main RFP\n","                    main_link = links[0]\n","                    main_title = main_link.text.strip()\n","                    main_url = main_link.get_attribute(\"href\")\n","\n","                    # Additional documents/info\n","                    additional_docs = []\n","                    additional_urls = []\n","                    for link in links[1:]:\n","                        doc_title = link.text.strip()\n","                        if doc_title:\n","                            additional_docs.append(doc_title)\n","                            additional_urls.append(link.get_attribute(\"href\"))\n","\n","                    if main_title:\n","                        bid_data = {\n","                            \"Type\": \"RFP/RFQ\",\n","                            \"Title\": main_title,\n","                            \"Due Date\": cells[1].text.strip() if len(cells) > 1 else \"\",\n","                            \"Additional Info\": \" | \".join(additional_docs) if additional_docs else \"\",\n","                            \"Details URL\": main_url + (\" | \" + \" | \".join(additional_urls) if additional_urls else \"\"),\n","                            \"Last Updated\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","                        }\n","                        bids.append(bid_data)\n","                        print(f\"Parsed RFP/RFQ: {bid_data['Title']}\")\n","\n","        except Exception as e:\n","            print(f\"Error parsing RFP table: {str(e)}\")\n","        return bids\n","\n","    def setup_csv(self):\n","        \"\"\"Create or verify CSV file with headers\"\"\"\n","        try:\n","            if not os.path.exists(self.output_file):\n","                with open(self.output_file, 'w', newline='', encoding='utf-8') as f:\n","                    writer = csv.DictWriter(f, fieldnames=self.fieldnames)\n","                    writer.writeheader()\n","                print(f\"Created new CSV file: {self.output_file}\")\n","            else:\n","                print(f\"CSV file already exists: {self.output_file}\")\n","        except Exception as e:\n","            print(f\"Error setting up CSV: {str(e)}\")\n","\n","    def get_page_with_retry(self):\n","        \"\"\"Attempt to load the page with retries\"\"\"\n","        for attempt in range(self.max_retries):\n","            try:\n","                print(f\"\\nAttempt {attempt + 1} to load page...\")\n","                self.driver.get(self.base_url)\n","                self.random_delay()\n","\n","                # Check if page loaded successfully by looking for either a table or bid results\n","                try:\n","                    self.wait.until(\n","                        EC.presence_of_element_located((By.CSS_SELECTOR, \"table, h2\"))\n","                    )\n","                except Exception as e:\n","                    print(f\"Error waiting for page load: {str(e)}\")\n","                    continue\n","\n","                print(\"Page loaded successfully\")\n","                return True\n","\n","            except Exception as e:\n","                print(f\"Error loading page: {str(e)}\")\n","                if attempt < self.max_retries - 1:\n","                    wait_time = (attempt + 1) * 5\n","                    print(f\"Waiting {wait_time} seconds before retry...\")\n","                    time.sleep(wait_time)\n","                continue\n","        return False\n","\n","    def get_bid_listings(self):\n","        \"\"\"Fetch and parse all bid listings\"\"\"\n","        try:\n","            if not self.get_page_with_retry():\n","                print(\"Failed to load page after all retries\")\n","                return []\n","\n","            print(\"Looking for bid listings...\")\n","            bids = []\n","\n","            # Parse bid results\n","            print(\"\\nParsing bid results section...\")\n","            bid_results = self.parse_bid_results()\n","            bids.extend(bid_results)\n","\n","            # Parse RFP/RFQ table\n","            print(\"\\nParsing RFP/RFQ table...\")\n","            rfp_bids = self.parse_rfp_table()\n","            bids.extend(rfp_bids)\n","\n","            print(f\"Successfully parsed {len(bids)} total bids\")\n","            return bids\n","\n","        except Exception as e:\n","            print(f\"Error fetching bid listings: {str(e)}\")\n","            return []\n","\n","    def save_bids(self, bids):\n","        \"\"\"Save bid data to CSV\"\"\"\n","        try:\n","            if not bids:\n","                print(\"No bids to save\")\n","                return\n","\n","            existing_bids = set()\n","            if os.path.exists(self.output_file):\n","                with open(self.output_file, 'r', encoding='utf-8') as f:\n","                    reader = csv.DictReader(f)\n","                    for row in reader:\n","                        existing_bids.add(f\"{row['Type']}-{row['Title']}-{row['Due Date']}\")\n","\n","            new_bids = [\n","                bid for bid in bids\n","                if f\"{bid['Type']}-{bid['Title']}-{bid['Due Date']}\" not in existing_bids\n","            ]\n","\n","            if new_bids:\n","                mode = 'w' if not os.path.exists(self.output_file) else 'a'\n","                with open(self.output_file, mode, newline='', encoding='utf-8') as f:\n","                    writer = csv.DictWriter(f, fieldnames=self.fieldnames)\n","                    if mode == 'w':\n","                        writer.writeheader()\n","                    writer.writerows(new_bids)\n","                print(f\"Added {len(new_bids)} new bids\")\n","            else:\n","                print(\"No new bids to add\")\n","\n","            # Download the CSV file\n","            files.download(self.output_file)\n","\n","        except Exception as e:\n","            print(f\"Error saving bids: {str(e)}\")\n","\n","    def run(self):\n","        \"\"\"Main execution method\"\"\"\n","        try:\n","            print(f\"Starting La Mirada bids crawler at {datetime.now()}\")\n","            self.setup_csv()\n","            bids = self.get_bid_listings()\n","            self.save_bids(bids)\n","            print(\"Crawler execution completed\")\n","        finally:\n","            if hasattr(self, 'driver'):\n","                self.driver.quit()"],"metadata":{"id":"1KHfLBzmMaWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["crawler = LaMiradaBidsCrawler()\n","crawler.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":805},"id":"qVyRypMkMad6","executionInfo":{"status":"ok","timestamp":1740079863893,"user_tz":300,"elapsed":6757,"user":{"displayName":"Ritunjay Murali","userId":"07921326813984298862"}},"outputId":"1a7ae1ec-f593-485d-d294-b594a848f354"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to use system chromedriver...\n","Chrome driver initialized successfully\n","Starting La Mirada bids crawler at 2025-02-20 19:30:57.767410\n","CSV file already exists: la_mirada_bids.csv\n","\n","Attempt 1 to load page...\n","Page loaded successfully\n","Looking for bid listings...\n","\n","Parsing bid results section...\n","Error parsing bid results: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"h2.titlewidget-title\"}\n","  (Session info: chrome=133.0.6943.126); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n","Stacktrace:\n","#0 0x5a3d13a34bea <unknown>\n","#1 0x5a3d134d27d0 <unknown>\n","#2 0x5a3d13523cc0 <unknown>\n","#3 0x5a3d13523e41 <unknown>\n","#4 0x5a3d13572984 <unknown>\n","#5 0x5a3d13549abd <unknown>\n","#6 0x5a3d1356fd0c <unknown>\n","#7 0x5a3d13549863 <unknown>\n","#8 0x5a3d13515ac8 <unknown>\n","#9 0x5a3d13516c31 <unknown>\n","#10 0x5a3d139fe18b <unknown>\n","#11 0x5a3d13a02112 <unknown>\n","#12 0x5a3d139eb04c <unknown>\n","#13 0x5a3d13a02d04 <unknown>\n","#14 0x5a3d139cf4bf <unknown>\n","#15 0x5a3d13a23528 <unknown>\n","#16 0x5a3d13a236f9 <unknown>\n","#17 0x5a3d13a33a66 <unknown>\n","#18 0x7946695fcac3 <unknown>\n","\n","\n","Parsing RFP/RFQ table...\n","Parsed RFP/RFQ: Request for Proposals for Website Design Services\n","Parsed RFP/RFQ: Request for Proposals for Landscape Mtc Services\n","Parsed RFP/RFQ: Request for Proposals for Curb Numbering Services\n","Parsed RFP/RFQ: Request for Proposals for July 3 Fireworks Display\n","Parsed RFP/RFQ: Request for Proposals for Sound Engineering Services\n","Successfully parsed 5 total bids\n","No new bids to add\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_f0474f35-e393-4a95-9292-f52b14d7cf76\", \"la_mirada_bids.csv\", 3448)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Crawler execution completed\n"]}]},{"cell_type":"markdown","source":["## Disclaimer\n","This tool accesses publicly available information only from official government websites. It respects robots.txt guidelines and implements responsible scraping practices with delays between requests."],"metadata":{"id":"tMExmprJKmRx"}}]}